\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a2f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

\begin{document}


\title{CPSC 340 Assignment 4 (due Friday, March 9 at 9:00pm)}
\date{}
\maketitle

\vspace{-7em}

\section*{Instructions}
\rubric{mechanics:3}

The above points are allocated for following the general homework instructions. In addition to the usual instructions,
\textbf{we have a NEW REQUIREMENT for this assignment} (and future assignments, unless it's a disaster):
if you're embedding your answers in a document that also contains the questions,
your answers should be in \blu{blue text}. This should hopefully make it much easier for the grader to find
your answers. To make something blue, you can use the LaTeX macro \verb|\blu{my text}|.


\section{Convex Functions}
\rubric{reasoning:5}

Recall that convex loss functions are typically easier to minimize than non-convex functions, so it's important to be able to identify whether a function is convex. 

\blu{Show that the following functions are convex}:

\enum{
\item $f(w) = \alpha w^2 - \beta w + \gamma$ with $w \in \R, \alpha \geq 0, \beta \in \R, \gamma \in \R$ (1D quadratic).\\
\blu{$f'(w)=2\alpha w - \beta$\\
$f''(w)=2\alpha \geq 0$}
\item $f(w) = w\log(w) $ with $w > 0$ (``neg-entropy'')\\
\blu{$f'(w)=1 + \log(w)$\\
	$f''(w)=\frac{1}{w} \geq 0$}
\item $f(w) = \norm{Xw-y}^2 + \lambda\norm{w}_1$ with $w \in \R^d, \lambda \geq 0$ (L1-regularized least squares).
\blu{$\norm{Xw-y}^2$ is convex function with respect to w because it is a L2 norm and norms are convex.}\\
 $\lambda\norm{w}_1$ is also convex since L1 norm is convex and $\lambda$ is positive. The sum of two convex function is also convex.\\
\item $f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i)) $ with $w \in \R^d$ (logistic regression).\\
 \blu{Sum of convex functions is convex. We just need to show $f(w) = \log(1+\exp(-y_iw^Tx_i)) $ is convex.
$ -y_iw^Tx_i $ is linear. It doesn't affect the convexity. Let $z= -y_iw^Tx_i $. The problem goes to check the convexity of $ g(z)=\log(1+\exp(z)) $.\\
 $g(z)'= \frac{\exp(z)}{1+\exp(z)}=\frac{1}{1+exp(-z)}\\
 g(z)''=\frac{\exp(-z)}{(1+\exp(-z))^2}\geq0$ Since exponential function and quadratic function are alway positive.
Thus the original function is convex function. }
\item $f(w,w_0) = \sum_{i=1}^N[\max\{0,w_0 - w^Tx_i\} - w_0] + \frac{\lambda}{2}\norm{w}_2^2$  with $w \in R^d, w_0 \in \R, \lambda \geq 0$ (``1-class'' SVM).
\blu{ $w_0 - w^Tx_i$ is convex function. 0 is constant, and it is convex. maximum of two convex function is convex. $w0$ is convex function. summation of convex functions is convex function so the first term is convex. L2 norm is convex. $\lambda$ is positive, so the second term is convex too. By summing two convex function, the original function is convex function.}
}


\section{Logistic Regression with Sparse Regularization}

If you run  \verb|python main.py -q 2|, it will:
\enum{
\item Load a binary classification dataset containing a training and a validation set.
\item `Standardize' the columns of $X$ and add a bias variable (in \emph{utils.load\_dataset}).
\item Apply the same transformation to $Xvalidate$ (in \emph{utils.load\_dataset}).
\item Fit a logistic regression model.
\item Report the number of features selected by the model (number of non-zero regression weights).
\item Report the error on the validation set.
}
Logistic regression does ok on this dataset,
but it uses all the features (even though only the prime-numbered features are relevant)
and the validation error is above the minimum achievable for this model
(which is 1 percent, if you have enough data and know which features are relevant).
In this question, you will modify this demo to use different forms of regularization
 to improve on these aspects.

Note: your results may vary a bit depending on versions of Python and its libraries.


\subsection{L2-Regularization}
\rubric{code:2}

Make a new class, \emph{logRegL2}, that takes an input parameter $\lambda$ and fits a logistic regression model with L2-regularization. Specifically, while \emph{logReg} computes $w$ by minimizing
\[
f(w) = \sum_{i=1}^n \log(1+\exp(-y_iw^Tx_i)),
\]
your new function \emph{logRegL2} should compute $w$ by minimizing
\[
f(w) = \sum_{i=1}^n \left[\log(1+\exp(-y_iw^Tx_i))\right] + \frac{\lambda}{2}\norm{w}^2.
\]
\blu{Hand in your updated code. Using this new code with $\lambda = 1$, report how the following quantities change: the training error, the validation error, the number of features used, and the number of gradient descent iterations.}

\blu{logRegL2 Training error 0.002\\
logRegL2 Validation error 0.074\\
 nonZeros: 101\\
The training error increases, validation error decreases, numbers of features are the same.
The number of iterations is 36.\\
\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}
}



\subsection{L1-Regularization}
\rubric{code:3}

Make a new class, \emph{logRegL1}, that takes an input parameter $\lambda$ and fits a logistic regression model with L1-regularization,
\[
f(w) = \sum_{i=1}^n \left[\log(1+\exp(-y_iw^Tx_i))\right] + \lambda\norm{w}_1.
\]
\blu{Hand in your updated code. Using this new code with $\lambda = 1$, report how the following quantities change: the training error, the validation error, the number of features used, and the number of gradient descent iterations.
\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}}


You should use the function \emph{minimizers.findMinL1}, which implements a
proximal-gradient method to minimize the sum of a differentiable function $g$ and $\lambda\norm{w}_1$,
\[
f(w) = g(w) + \lambda \norm{w}_1.
\]
This function has a similar interface to \emph{findMin}, \textbf{EXCEPT} that (a) you
only pass in the the function/gradient of the differentiable
part, $g$, rather than the whole function $f$; and (b) you need to provide the value $\lambda$.
Thus, your \texttt{funObj} shouldn't actually contain the L1 regularization, since it's implied
in the way you express your objective to the optimizer. 

\blu{logRegL1 Training error 0.000  It decreses\\
logRegL1 Validation error 0.052 It increases\\
nonZeros: 71. It decreses!!\\
number of iteration is 78. It increses.
}


\subsection{L0-Regularization}
\rubric{code:4}

The class \emph{logRegL0} contains part of the code needed to implement the \emph{forward selection} algorithm,
which approximates the solution with L0-regularization,
\[
f(w) =  \sum_{i=1}^n \left[\log(1+\exp(-y_iw^Tx_i))\right] + \lambda\norm{w}_0.
\]
The \texttt{for} loop in this function is missing the part where we fit the model using the subset \emph{selected\_new},
then compute the score and updates the \emph{minLoss/bestFeature}.
Modify the \texttt{for} loop in this code so that it fits the model using only
the features \emph{selected\_new}, computes the score above using these features,
and updates the \emph{minLoss/bestFeature} variables.
\blu{Hand in your updated code. Using this new code with $\lambda=1$,
report the training error, validation error, and number of features selected.\\
\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}}

\blu{Training error: 0.000\\
Validation error: 0.032\\
 nonZeros: 24 = number of features is 24 }
 



\subsection{Discussion}
\rubric{reasoning:2}

In a short paragraph, briefly discuss your results from the above. How do the
different forms of regularization compare with each other?
Can you provide some intuition for your results? No need to write a long essay, please!

\blu{L1 regularization can do variable selection. L0 also can do more job of variable selection. L2 regularization cannot do that. In terms of curing overfitting, L2 is not good. But L0 takes a long time for doing nested loops.}


\subsection{Comparison with scikit-learn}
\rubric{reasoning:1}

Compare your results (training error, validation error, number of nonzero weights) for L2 and L1 regularization with scikit-learn's LogisticRegression. Use the
\texttt{penalty} parameter to specify the type of regularization. The parameter \texttt{C} corresponds to $\frac{1}{\lambda}$, so if
you had $\lambda=1$ then use \texttt{C=1} (which happens to be the default anyway).
You should set \texttt{fit\string_intercept} to \texttt{False} since we've already added the column of ones to $X$ and thus
there's no need to explicitly fit an intercept parameter. After you've trained the model, you can access the weights
with \texttt{model.coef\string_}.

\blu{
L1:
Training error 0.000\\
Validation error 0.052\\
number of nonZeros: 71\\
It is the same as my previous L1 regularization\\
L2:
Training error 0.002\\
Validation error 0.074\\
number of nonZeros: 101\\
It is the same as my previous L1 regularization
\\ But numbers of features are the same. 
\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}}

\section{Multi-Class Logistic}

If you run \verb|python main.py -q 3| the code loads a multi-class
classification dataset with $y_i \in \{0,1,2,3,4\}$ and fits a `one-vs-all' classification
model using least squares, then reports the validation error and shows a plot of the data/classifier.
The performance on the validation set is ok, but could be much better.
For example, this classifier never even predicts that examples will be in classes 0 or 4.


\subsection{Softmax Classification, toy example}
\rubric{reasoning:2}

Linear classifiers make their decisions by finding the class label $c$ maximizing the quantity $w_c^Tx_i$, so we want to train the model to make $w_{y_i}^Tx_i$ larger than $w_{c'}^Tx_i$ for all the classes $c'$ that are not $y_i$. 
Here $c'$ is a possible label and $w_{c'}$ is row $c'$ of $W$. Similarly, $y_i$ is the training label, $w_{y_i}$ is row $y_i$ of $W$, and in this setting we are assuming a discrete label $y_i \in \{1,2,\dots,k\}$. Before we move on to implementing the softmax classifier to fix the issues raised in the introduction, let's work through a toy example:

Consider the dataset below, which has $n=10$ training examples, $d=2$ features, and $k=3$ classes:
\[
X = \begin{bmatrix}0 & 1\\1 & 0\\ 1 & 0\\ 1 & 1\\ 1 & 1\\ 0 & 0\\  1 & 0\\  1 & 0\\  1 & 1\\  1 &0\end{bmatrix}, \quad y = \begin{bmatrix}1\\1\\1\\2\\2\\2\\3\\3\\3\\3\end{bmatrix}.
\]
Suppose that you want to classify the following test example:
\[
\hat{x} = \begin{bmatrix}1 & 1\end{bmatrix}.
\]
Suppose we fit a multi-class linear classifier using the softmax loss, and we obtain the following weight matrix:
\[
W =
\begin{bmatrix}
+2 & -1\\
+2 & +2\\
+3 & -1
\end{bmatrix}
\]
\blu{Under this model, what class label would we assign to the test example? (Show your work.)}

\blu{$W\hat{x^T}=\begin{bmatrix}
+2 & -1\\
+2 & +2\\
+3 & -1
\end{bmatrix} \times
\begin{bmatrix}
1 \\
1\\
\end{bmatrix}
=\begin{bmatrix}
1 \\
4\\
2
\end{bmatrix}$ We assign the test example to the second label.}
\subsection{One-vs-all Logistic Regression}
\rubric{code:2}

Using the squared error on this problem hurts performance because it has `bad errors' (the model gets penalized if it classifies examples `too correctly').
Write a new class, \emph{logLinearClassifier}, that replaces the squared loss in the one-vs-all model with the logistic loss. \blu{Hand in the code and report the validation error}.

\blu{\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}\\
logLinearClassifier Training error 0.084\\
logLinearClassifier Validation error 0.070\\
\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}}

\subsection{Softmax Classifier Implementation}
\rubric{code:5}

Using a one-vs-all classifier hurts performance because the classifiers are fit independently, so there is no attempt to calibrate the columns of the matrix $W$. An alternative to this independent model is to use the softmax loss, which is given by
\[
f(W) = \sum_{i=1}^n \left[-w_{y_i}^Tx_i + \log\left(\sum_{c' = 1}^k \exp(w_{c'}^Tx_i)\right)\right] \, ,
\]

The partial derivatives of this function, which make up its gradient, are given by

\[
\frac{\partial f}{\partial W_{cj}} = \sum_{i=1}^n x_{ij}[p(y_i=c | W,x_i) - I(y_i = c)] \, ,
\]
where... 
\begin{itemize}
\item $I(y_i = c)$ is the indicator function (it is $1$ when $y_i=c$ and $0$ otherwise)
\item $p(y_i=c | W, x_i)$ is the predicted probability of example $i$ being class $c$, defined as
\[
p(y_i | W, x_i) = \frac{\exp(w_{y_i}^Tx_i)}{\sum_{c'=1}^k\exp(w_{c'}^Tx_i)}
\]

\end{itemize}


(Good news: in previous offerings of CPSC 340, you had to derive this! I think you've probably taken enough derivatives by now though.)

Make a new class, \emph{softmaxClassifier}, which fits $W$ using the softmax loss from the previous section instead of fitting $k$ independent classifiers. \blu{Hand in the code and report the validation error
\url{https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/k6y1b_a4/blob/master/code/linear_model.py}}.

 \blu{Validation error 0.008}

Hint: you may want to use \verb|utils.check_gradient| to check that your implementation of the gradient is correct.



\subsection{Comparison with scikit-learn, again}
\rubric{reasoning:1}

Compare your results (training error and validation error for both one-vs-all and softmax) with scikit-learn's \texttt{LogisticRegression}, 
which can also handle multi-class problems. 
One-vs-all is the default; for softmax, set \texttt{multi\string_class='multinomial'}. For the softmax case,
you'll also need to change the solver. You can use \texttt{solver='lbfgs'}.
Since your comparison code above isn't using regularization, set \texttt{C} very large to effectively disable regularization. 
Again, set \texttt{fit\string_intercept} to \texttt{False} for the same reason as above.

\blu{Training error 0.084\\
Validation error 0.070\\
number of nonZeros: 15\\
Training error 0.000\\
Validation error 0.012\\
number of  nonZeros: 15}

\subsection{Cost of Multinomial Logistic Regression}
\rubric{reasoning:2}

Assuming that we have
\items{
\item $n$ training examples.
\item $d$ features.
\item $k$ classes.
\item $t$ testing examples.
\item $T$ iterations of gradient descent for training.
}
\blu{\enum{
\item In $O()$ notation, what is the cost of training the softmax classifier?\\
 O(ndkT).
\item What is the cost of classifying the test examples?\\
 O(tdk).
}}




\section{Very-Short Answer Questions}
\rubric{reasoning:9}

\enum{
\item Why would you use a score BIC instead of a validation error for feature selection?
\blu{Using validation error is suspectible to choosing a model due to optimization bias when a large number of models are compared. BIC compares model based penalized likelihood, which does not have this problem.}
\item Why do we use forward selection instead of exhaustively search all subsets in search and score methods?
\blu{Exhaustive search cost is too expensive. Forward selection is less likely to make false positive.}
\item In L2-regularization, how does $\lambda$ relate to the two parts of the fundamental trade-off?
\blu{$\lambda$ increase, then it penalize more, so larger training error but lower approximation error. }
\item Give one reason why one might chose to use L1 regularization over L2 and give one reason for the reverse case.
\blu{L1 regularization is likely to give sparse solutions, can help variable selection. L2 regularization has a unique solution and can be analytically computed, Insensitive to changes in data. }
\item What is the main problem with using least squares to fit a linear model for binary classification?\\
\blu{Even the sign is the same, the model will penalize a lot if the absolute value of a evaluated value is much larger then the true label value. That is, it will penalize a lot for very right values if the value($w^Tx_i$) is much greater than 1 or even less then -1. }
\item For a linearly separable binary classification problem, how does a linear SVM differ from a classifier found using the perceptron algorithm?\\
\blu{The SVM classifier returns the maximum-margin classifier. That is, it maximize distance to the training examples. On the other hand, the perceptron method could return any classifier.}
\item Which of the following methods produce linear classifiers? (a) binary least squares as in Question 3, (b) the perceptron algorithm, (c) SVMs, and (d) logistic regression.
\blu{They are all linear classifiers. The performance can be different from each model.}
\item What is the difference between multi-label and multi-class classification?\\
\blu{multi class classification is it has more than two classes for its response value.  
Multi label classification is the case that the situation is multi class classificaiton and at the same time, each object can have more than 1 label for its label. 
}
\item Fill in the question marks: for one-vs-all multi-class logistic regression, we are solving $k$ optimization problem(s) of dimension $d$. On the other hand, for softmax logistic regression, we are solving $1$ optimization problem(s) of dimension $d*k$.\\
\blu{k,d,1,d*k}
}


Hints: we're looking for short and concise 1-sentence answers, not long and complicated answers. Also, there is roughly 1 question per lecture.


\end{document}
